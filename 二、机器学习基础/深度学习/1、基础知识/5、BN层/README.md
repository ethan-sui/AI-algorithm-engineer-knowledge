## BN层
&emsp;&emsp;在神经网络中，数据经过每一层时，都会有可能引起数据分布的变化，而使得收敛较慢，所有BN层的思路就是想着在每一层后面加一个标准化的擦欧总，再送入下一层进行学习，但如果只是简单的进行归一化，又可能会导致某些数据特征被破坏，比如某层使用ReLU激活函数，然后输入数据都落在0的左侧，那这个时候都被激活了，但如果使用标准化，那将有很多的参数跑到左侧，从而这部分特征就学不到了，所以BN层又使用了两个可学习的参数，来进行缓和，即
$y^{(k)}=\gamma ^{(k)}\hat{x}^{(k)}+\beta ^{(k)}$
